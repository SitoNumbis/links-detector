{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_objects_detector_v5__public.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C6RmYsKrEvV"
      },
      "source": [
        "# üìñ üëÜüèª Printed Links Detection Using TensorFlow 2 Object Detection API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNeiUTuPa-K7"
      },
      "source": [
        "![Links Detector Cover](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/01-banner.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIkPgKIWsHiO"
      },
      "source": [
        "## üìÉ TL;DR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJedZpEkSKYq"
      },
      "source": [
        "_In this article we will start solving the issue of making the printed links (i.e. in a book or in a magazine) clickable via your smartphone camera._\n",
        "\n",
        "We will use TensorFlow 2 [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to train a custom object detector model to find positions and bounding boxes of the sub-strings like `https://` in the text image (i.e. in smartphone camera stream).\n",
        "\n",
        "The text of each link (right continuation of `https://` bounding box) will be recognized by using [Tesseract](https://tesseract.projectnaptha.com/) library. The recognition part will not be covered in this article but you may find the complete code example of the application in [links-detector repository](https://github.com/trekhleb/links-detector).   \n",
        "\n",
        "> üöÄ [**Launch Links Detector demo**](https://trekhleb.github.io/links-detector/) from your smartphone to see the final result.\n",
        "\n",
        "> üìù [**Open links-detector repository**](https://github.com/trekhleb/links-detector) on GitHub to see the complete source code of the application.\n",
        "\n",
        "Here is how the final solution will look like:\n",
        "\n",
        "![Links Detector Demo](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/03-links-detector-demo.gif)\n",
        "\n",
        "> ‚ö†Ô∏è Currently the application is in _experimental_ _Alpha_ stage and has [many issues and limitations](https://github.com/trekhleb/links-detector/issues?q=is%3Aopen+is%3Aissue+label%3Aenhancement). So don't raise your expectations bar to high until these issues are resolved ü§∑üèª‚Äç. Also the pruspose of this article is more about learning how to work with TensorFlow 2 Object Detection API rather than comming up with a production ready model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlkOLNbOsajd"
      },
      "source": [
        "## ü§∑üèª‚Äç‚ôÇÔ∏è The Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucfFZFofSSTx"
      },
      "source": [
        "I work as a software engineer and on my own time I learn Machine Learning as a hobby. But this is not the problem yet.\n",
        "\n",
        "I bought a printed book about Machine Learning recently and while I was reading through the first several chapters I've encountered many printed links in the text that looked like `https://tensorflow.org/` or `https://some-url.com/which/may/be/even/longer?and_with_params=true`.\n",
        "\n",
        "![Printed Links](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/02-printed-links.jpg)\n",
        "\n",
        "I saw all these links but I couldn't click on them since they were printed (thanks, cap!). To visit these links I needed to start typing them character by character in the browser's address bar, which was pretty annoying and error prone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj7Gc-KVs4q6"
      },
      "source": [
        "## üí° Possible Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob8k5gcfixDd"
      },
      "source": [
        "So, what if, similarly to QR-code detection, we will try to \"teach\" the smartphone to _(1)_ _detect_ and _(2)_ _recognize_ printed links for us and also to make them _clickable_? This way you'll do just one click instead of multiple keystrokes. The operational complexity goes from `O(N)` to `O(1)`.\n",
        "\n",
        "This is how the final workflow will look like:\n",
        "\n",
        "![Links Detector Demo](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/03-links-detector-demo.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfcnUFjcVDAy"
      },
      "source": [
        "## üìù Solution Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h2q_AQ_VMko"
      },
      "source": [
        "As I've mentioned earlier I'm just studying a Machine Learning as a hobby. Thus the pruspose of this article is more about _learning_ how to work with TensorFlow 2 Object Detection API rather than comming up with a production ready application.\n",
        "\n",
        "With that beign said, I simplified the solution requirements to the following:\n",
        "\n",
        "1. The detection and recognition processes should have a **close-to-real-time** performance (i.e. `0.5-1` frames per second) on a device like iPhone X. It means that whole _detection + recognition_ process should take up to `2` seconds (preatty bearable as for the amateur project).\n",
        "2. Only **English** links should be supported.\n",
        "3. Only **dark text** (i.e. black or dark-grey) on **light background** (i.e. white or light-grey) should be supported.\n",
        "4. Only `https://` links should be supported for now (it is ok if our model will not recognize the `http://`, `ftp://`, `tcp://` or other types of links)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4orlNJkZs7vB"
      },
      "source": [
        "## üß© Solution Breakdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VoRXizCNC-d"
      },
      "source": [
        "### High-level breakdown\n",
        "\n",
        "Let's see how we could approach the problem on a high level.\n",
        "\n",
        "#### Option 1: Detection model on the back-end\n",
        "\n",
        "**The flow:**\n",
        "\n",
        "1. Get camera stream (frame by frame) on the client side.\n",
        "2. Send each frame one by one over the network to the back-end.\n",
        "3. Do links detection and recognition on the back-end and send the response back to the client.\n",
        "4. Client draws the detection boxes with the clickable links.\n",
        "\n",
        "![Model on the back-end](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/04-frontend-backend.jpg)\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "üíö The detection performance is not limited by the client's device. We may speed the detection up by scaling the service horizontally (adding more instances) and vertically (adding more cores/GPUs).\n",
        "\n",
        "üíö The model might be bigger since there is no need to upload it to the client side. Downloading the `~10Mb` model on the client side may be ok, but loading the `~100Mb` model might be a big issue for the client's network and application UX (user experience) otherwise.\n",
        "\n",
        "üíö It is possible to controll who is using the model. Model is guarded behind the API so we would have complete controll over its callers/clients.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "üíî System complexity growth. The aplication tech stack growth from just `JavaScript` to, let's say, `JavaScript + Python`. We need to take care about the autoscaling.\n",
        "\n",
        "üíî Offline mode for the app is not possible since it needs an internet connection to work.\n",
        "\n",
        "üíî Too many HTTP requests between the client and the server may become a bottleneck at some point. Imagine if we would want to improve the performance of the detecton, let's say, from `1` to `10+` frames per second. This means that each client will send `10+` requests per second. For `10` simultanious clients it is already `100+` requests per second. The `HTTP/2` bidirectional streaming and `gRPC` might be useful in this case, but we're going back to increased system complexity here.  \n",
        "\n",
        "üíî System becomes more expensive. Almost all points from Pros section need to be paid for.\n",
        "\n",
        "#### Option 2: Detection model on the front-end\n",
        "\n",
        "**The flow:**\n",
        "\n",
        "1. Get camera stream (frame by frame) on the client side.\n",
        "2. Do links detection and recognition on the client side (without sending anything to the back-end).\n",
        "3. Client draws the detection boxes with the clickable links.\n",
        "\n",
        "![Model on the front-end](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/05-frontend-only.jpg)\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "üíö System is less complex. We don't need to set up the servers, build the API and introcude an additional Python stack to the system. \n",
        "\n",
        "üíö Offline mode is possible. The app doesn't need an internet connection to work since the model is loaded on the device. So the Progressive Web Application ([PWA](https://web.dev/progressive-web-apps/)) might be built to support that.\n",
        "\n",
        "üíö System is \"kind of\" scaling automatically. The more clients you have, the more cores and GPUs they bring. This is not a proper scaling solution though (more about that in a Cons section below). \n",
        "\n",
        "üíö System is cheaper. We only need a server for static assets (`HTML`, `JS`, `CSS`, model files etc.). This may be done for free, let's say, on GitHub.\n",
        "\n",
        "üíö No issue with the growing number of HTTP requests per second to the server side.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "üíî Only the horizontal scaling is possible (each client will have it's own CPU/GPU). Vertical scaling is not possible since we can't influence the client's device performance. As a result we can't guarantee fast detection for low performant devices.\n",
        "\n",
        "üíî It is not possible to guard the model usage and controll the callers/clients of the model. Everyone could download the model and re-use it. \n",
        "\n",
        "üíî Battery consumption of the client's device might become an issue. For the model to work it needs computational resources. So clients might not be happy with their iPhone getting warmer and warmer while the app is working.\n",
        "\n",
        "#### High-level conslusion\n",
        "\n",
        "Since the purpose of the project was more about learning and not comming up with a production ready solution _I decided to go with the second option of serving the model from the client side_. This made the whole project much cheaper (actually with the GitHub it was free to host it) and I could focus more on Machine Learning then on the autoscaling back-end infrastructure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KQrN1ICEBpR"
      },
      "source": [
        "### Lower level breakdown\n",
        "\n",
        "Ok, so we've decided to go with the serverless solution. And now we have an image from the camera stream as an input that looks something like this:\n",
        "\n",
        "![Printed Links Input](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/06-printed-links-clean.jpg)\n",
        "\n",
        "We need to solve two sub-tasks for this image:\n",
        "\n",
        "1. Links **detection** (finding the position and bounding boxes of the links)\n",
        "2. Links **recognition** (recognizing the text of the links)\n",
        "\n",
        "#### Option 1: Tesseract based solution\n",
        "\n",
        "The first and the most obvious aproach would be to solve the _Optical Character Recognition_ ([OCR](https://en.wikipedia.org/wiki/Optical_character_recognition)) task by recognizing the whole text of the image by using, let's say, [Tesseract.js](https://github.com/naptha/tesseract.js) library. As a pleasent bonus it returns the bounding boxes of the paragraphs, text lines and text blocks along with the recognized text.\n",
        "\n",
        "![Recognized text with bounding boxes](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/07-printed-links-boxes.jpg)\n",
        "\n",
        "Then we may try to extract the links from the recognized text lines or text blocks with a regular expression like [this one](https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url):\n",
        "\n",
        "```typescript\n",
        "const URL_REG_EXP = /https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_+.~#?&/=]*)/gi;\n",
        "\n",
        "const extractLinkFromText = (text: string): string | null => {\n",
        "  const urls: string[] | null = text.match(URL_REG_EXP);\n",
        "  if (!urls || !urls.length) {\n",
        "    return null;\n",
        "  }\n",
        "  return urls[0];\n",
        "};\n",
        "```\n",
        "\n",
        "üíö Seems like the issue is solved in a pretty straightforward and simple way:\n",
        "\n",
        "- We know the bounding boxes of the links\n",
        "- And we also know the text of the links to make them clickable\n",
        "\n",
        "üíî The thing is that the _recognition + detection_ time may vary from `2` to `20+` seconds depending on the size of the text, on the ammount of \"something that looks like a text\" on the image, on the image quality and on other factors. So it will be realy hard to achive those `0.5-1` frames per second to make the user experience at least _close_ to the real-time.\n",
        "\n",
        "üíî Also if we would think about it, we're asking the library to recognize the **whole** text from the image for us even though it might contain only one or two links in it (i.e. only ~10% of the text might be usefull for us) or it may even not contain the links at all. In this case it sounds like a waste of the computational resources. \n",
        "\n",
        "#### Option 2: Tesseract + TensorFlow based solution\n",
        "\n",
        "We could make Tesseract work faster if we used some _additional \"adviser\" algorithm_ in prior to the links text recognition. This \"adviser\" algorithm should detect, but not recognize, _the the leftmost position_ of each link on the image if there are any. This will allow us to speed up the recognition part by following these rules:\n",
        "\n",
        "1. If the image does not contain any link we should not call Tesseract detection/recognition at all.\n",
        "2. If the image does have the links then we need to ask Tesseract to recognize only those parts of the image that contains the links. We're not interested in spending time for recognition of the irrelevant text that doesn't contain the links.\n",
        "\n",
        "The \"adviser\" algorithm that will take place before the Tesseract should work with a constant time regardless of the image quality or the presence/absence of the text on the image. It also should be pretty fast and detect the leftmost positions of the links for less then `1s` so that we could satisfy the \"close-to-real-time\" requirement (i.e. on iPhone X).\n",
        "\n",
        "> üí° So what if we will use another object detection model to help us find all occurrences of the `https://` substrings (every secure link has this prefix, doesn't it) in the image? Then, having these `https://` bounding boxes in the text we may extract the right-side continuation of them and send them to the Tesseract for text recognition.\n",
        "\n",
        "Take a look at the picture below:\n",
        "\n",
        "![Tesseract and TensorFlow based solution](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/08-tesseract-vs-tensorflow.jpg)\n",
        "\n",
        "You may notice that Tesseract needs to do **much less** work in case if it would have some hints about where are the links might be located (see the number of blue boxes on both pictures).\n",
        "\n",
        "So the question now is which object detection model we should choose and how to re-train it to support the detection of the custom `https://` objects (to do the [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)).  \n",
        "\n",
        "> Finally! We've got closer to the TensorFlow part of the article üòÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJTrZirnHEwf"
      },
      "source": [
        "## ü§ñ Selecting the object detection model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGT_-SxZHng-"
      },
      "source": [
        "> I'm not a machine learning expert so please forgive me some future inaccuracies in this section in advance. Feel free to add corrections in the comments section under the article.\n",
        "\n",
        "`ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8`\n",
        "\n",
        ", **but** this \"adviser algorithm that will take place before the Tesseract\" reminds me about [Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) or SSD (I haven't read the paper though). The cool part about it is that it does the objects detection for the whole image and for all possible objects in it **in one go** regardless of the image content! \n",
        "\n",
        "\n",
        "We will use TensorFlow 2 [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to train a custom object detector model to find positions and bounding boxes of the sub-strings like `https://` in the text image (i.e. in smartphone camera stream)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJQpRyWcud-r"
      },
      "source": [
        "## üìù Creating the Dataset Manually\n",
        "\n",
        "- Making pictures of the book\n",
        "- What tools to use to add bounding boxes\n",
        "- How to convert to protobuf\n",
        "- Issues with custom dataset (fonts, colors, bolds, underlined, etc.)\n",
        "- Train/test split approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H9ubdH7ZQqv"
      },
      "source": [
        "### üåÖ Preprocessing the data\n",
        "\n",
        "- Data preprocessing: resize, crop square, color adjustment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_0iXNZPZWLz"
      },
      "source": [
        "### üîñ Labeling the dataset\n",
        "\n",
        "- How to use LabelImg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKZgmaG6Zfbb"
      },
      "source": [
        "### üóú Exporting the dataset\n",
        "\n",
        "- Protobuf (the way of storing the dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLAAnasyvBhS"
      },
      "source": [
        "## üìö Generating the Dataset Automatically (?)\n",
        "\n",
        "- Automated way of generating the dataset\n",
        "- Train/test split approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdk05ymSO7WK"
      },
      "source": [
        "## üìñ Exploring the Dataset\n",
        "\n",
        "- Preview images with detection boxes\n",
        "- Number of images (why is this enough)\n",
        "- Do we need to preprocess the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0t-B7net2gA"
      },
      "source": [
        "## üõ† Installing Object Detection API \n",
        "\n",
        "- What is object detection API\n",
        "- Why it will simplify our lives\n",
        "- How it may be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edimSsPB0eaJ",
        "outputId": "29046f3e-6e2d-4873-c9fc-b1fbd5df97ce"
      },
      "source": [
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1e2BNri16Nq",
        "outputId": "52d59145-5c68-4186-eea2-9009663e2b94"
      },
      "source": [
        "ls -la models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 72\n",
            "drwxr-xr-x  8 root root  4096 Nov 21 17:22 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x  1 root root  4096 Nov 21 17:24 \u001b[01;34m..\u001b[0m/\n",
            "-rw-r--r--  1 root root   337 Nov 21 17:22 AUTHORS\n",
            "-rw-r--r--  1 root root  1015 Nov 21 17:22 CODEOWNERS\n",
            "drwxr-xr-x  2 root root  4096 Nov 21 17:22 \u001b[01;34mcommunity\u001b[0m/\n",
            "-rw-r--r--  1 root root   390 Nov 21 17:22 CONTRIBUTING.md\n",
            "drwxr-xr-x  8 root root  4096 Nov 21 17:22 \u001b[01;34m.git\u001b[0m/\n",
            "drwxr-xr-x  3 root root  4096 Nov 21 17:22 \u001b[01;34m.github\u001b[0m/\n",
            "-rw-r--r--  1 root root  1104 Nov 21 17:22 .gitignore\n",
            "-rw-r--r--  1 root root  1115 Nov 21 17:22 ISSUES.md\n",
            "-rw-r--r--  1 root root 11405 Nov 21 17:22 LICENSE\n",
            "drwxr-xr-x 12 root root  4096 Nov 21 17:22 \u001b[01;34mofficial\u001b[0m/\n",
            "drwxr-xr-x  3 root root  4096 Nov 21 17:22 \u001b[01;34morbit\u001b[0m/\n",
            "-rw-r--r--  1 root root  3668 Nov 21 17:22 README.md\n",
            "drwxr-xr-x 23 root root  4096 Nov 21 17:22 \u001b[01;34mresearch\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfJpqVKg1aIH"
      },
      "source": [
        "%%bash\n",
        "cd ./models/research\n",
        "protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzM7civf12Gw",
        "outputId": "43ee10db-1784-4407-f3b6-1fa2d7aee6e0"
      },
      "source": [
        "%%bash\n",
        "cd ./models/research\n",
        "cp ./object_detection/packages/tf2/setup.py .\n",
        "pip install . --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: multiprocess 0.70.10 has requirement dill>=0.3.2, but you'll have dill 0.3.1.1 which is incompatible.\n",
            "ERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.0 which is incompatible.\n",
            "ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\n",
            "ERROR: apache-beam 2.25.0 has requirement avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\", but you'll have avro-python3 1.10.0 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQV0aWtuFpV"
      },
      "source": [
        "## ‚¨áÔ∏è Downloading Pre-Trained Model\n",
        "\n",
        "- Model detection Zoo review\n",
        "- What models we could use possibly\n",
        "- Why I've picked the MobileNet model\n",
        "- Diagram of the model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LpKaPIqPWph"
      },
      "source": [
        "## üèÑüèª‚Äç‚ôÇÔ∏è Trying the Model (Inference)\n",
        "\n",
        "- Show that model works for general purpose classes\n",
        "- Show that model doesn't work for custom objects (links)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7triRLeP-_K"
      },
      "source": [
        "## üìà Setting Up TensorBoard\n",
        "\n",
        "- Why do we need it (for debugging)\n",
        "- What we will monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhU74WMHQH1E"
      },
      "source": [
        "## üë®‚Äçüéì Transfer Learning\n",
        "\n",
        "- What is transfer learning\n",
        "- Why don't we train the model from scratch\n",
        "- Allows us to use small dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7nFZjHUQR4A"
      },
      "source": [
        "### ‚öôÔ∏è Configuring the Detection Pipeline\n",
        "\n",
        "- Performance issues: batch size\n",
        "- Starting not from scratch: checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA_QoEhzQHCT"
      },
      "source": [
        "### üèãüèª‚Äç‚ôÇÔ∏è Model Training\n",
        "\n",
        "- Error prone: saving checkpoints\n",
        "- How many epochs\n",
        "- Monitoring the performance while training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7OZozAHQklI"
      },
      "source": [
        "### üöÄ Evaluating the Model\n",
        "\n",
        "- Checking how accurate our model is on test dataset\n",
        "- Are we good with performance, should we save the model?\n",
        "- It is not a general purpose anymore, does it recognize our custom objects?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMiDAvqTQvg8"
      },
      "source": [
        "## üóú Exporting the Model\n",
        "\n",
        "- Saving the model to the file for further re-use\n",
        "- Show the list of files, how the model looks like on dics\n",
        "- What the size of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlliMpvuQyUz"
      },
      "source": [
        "## üöÄ Evaluating the Exported Model\n",
        "\n",
        "- Example of how to use the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Juyv3FMkQ4qO"
      },
      "source": [
        "## üóú Converting the Model for Web\n",
        "\n",
        "- What formats are sutable for the web\n",
        "- Few words about Tensorflow.js\n",
        "- Show list of exported files - how model looks like on disc\n",
        "- What the size of the model\n",
        "- Why it is split in chucnks and how they are connected (via model.json)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B8JU1UYzvBy",
        "outputId": "91b29075-0371-4018-e7a9-fbe08fc5055a"
      },
      "source": [
        "pip install tensorflowjs --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 10kB 26.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 20kB 12.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 30kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 51kB 4.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 4.1MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñè                            | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 20kB 20.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 30kB 16.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 40kB 14.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 51kB 11.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 61kB 11.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 71kB 7.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 81kB 8.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112kB 8.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhqXJkIpRF1A"
      },
      "source": [
        "## ü§î Conclusions\n",
        "\n",
        "- I'm just an amatour\n",
        "- Links to demo app\n",
        "- Issues and limitations of this approach\n",
        "- Links to my ML repositories that thy might like"
      ]
    }
  ]
}